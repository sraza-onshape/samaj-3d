{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4: Fun with Point Clouds?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Point Cloud Fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Harris Corner Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: allow Jupyter to \"hot reload\" the modules we import - after each change, rerun this cell (instead of restarting the kernel!!)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from util import ops\n",
    "from util.corner_detection import HarrisCornerDetector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../hw4_data/problem1/rgb1.png',\n",
       " '../hw4_data/problem1/rgb2.png',\n",
       " '../hw4_data/problem1/rgb3.png']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgb_paths = sorted(glob.glob(\"../hw4_data/problem1/rgb*\"))\n",
    "rgb_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of ../hw4_data/problem1/rgb1.png: 480 x 640\n",
      "Dimensions of ../hw4_data/problem1/rgb2.png: 480 x 640\n",
      "Dimensions of ../hw4_data/problem1/rgb3.png: 480 x 640\n"
     ]
    }
   ],
   "source": [
    "illumination_images = [\n",
    "    ops.load_image(\n",
    "        path,\n",
    "        return_grayscale=True,\n",
    "        return_array=True\n",
    "    )\n",
    "    for path in rgb_paths\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, let's map the filename of each RGB image, to the image itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's also load in the depth map images in a similar fashion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../hw4_data/problem1/depth1.png',\n",
       " '../hw4_data/problem1/depth2.png',\n",
       " '../hw4_data/problem1/depth3.png']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depthmap_paths = sorted(glob.glob(\"../hw4_data/problem1/depth*\"))\n",
    "depthmap_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of ../hw4_data/problem1/depth1.png: 480 x 640\n",
      "Dimensions of ../hw4_data/problem1/depth2.png: 480 x 640\n",
      "Dimensions of ../hw4_data/problem1/depth3.png: 480 x 640\n"
     ]
    }
   ],
   "source": [
    "depthmaps = [\n",
    "    ops.load_image(\n",
    "        path,\n",
    "        return_grayscale=True,\n",
    "        return_array=True\n",
    "    )\n",
    "    for path in depthmap_paths\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine Feature that Will Be Used to Fuse Point Clouds "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by setting the hyperparameters given in in the hw 4 description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivative_operator_x = np.array([\n",
    "    [-1, 0, 1],\n",
    "    [-1, 0, 1],\n",
    "    [-1, 0, 1],\n",
    "])\n",
    "derivative_operator_y = derivative_operator_x.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the Gaussian filter provided here: https://homepages.inf.ed.ac.uk/rbf/HIPR2/gsmooth.htm\n",
    "gaussian_window = np.array([\n",
    "    [1, 4, 7, 4, 1],\n",
    "    [4, 16, 26, 16, 4],\n",
    "    [7, 26, 41, 26, 7],\n",
    "    [4, 16, 26, 16, 4],\n",
    "    [1, 4, 7, 4, 1],\n",
    "])\n",
    "\n",
    "gaussian_window = gaussian_window * (1 / 273)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FEATURES_TO_SELECT = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's go ahead and apply the Harris Corner detector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_image_and_corners = dict()\n",
    "\n",
    "detector = HarrisCornerDetector()\n",
    "\n",
    "for rgb_path, image in zip(rgb_paths, illumination_images):\n",
    "\n",
    "    corner_response = detector.detect_features(\n",
    "        image,\n",
    "        use_non_max_suppression=True,\n",
    "        derivative_operator_x=derivative_operator_x,\n",
    "        derivative_operator_y=derivative_operator_y,\n",
    "        gaussian_window=gaussian_window,\n",
    "    )\n",
    "    # pick top features\n",
    "    top_k_points = detector.pick_top_features(\n",
    "        corner_response,\n",
    "        NUM_FEATURES_TO_SELECT\n",
    "    )\n",
    "\n",
    "    path_to_image_and_corners[rgb_path] = (image, top_k_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Corners to 3D Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by setting the hyperparameters given in in the hw 4 description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_matrix = K = np.array([\n",
    "    [525.,    0, 319.5],\n",
    "    [0,    525., 239.5],\n",
    "    [0,       0,     1]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can project out the 3D points corresponding to the detected corners:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linalg\n",
    "\n",
    "def convert_1_corner_to_3d(\n",
    "        xy_and_depth: tuple[int, int, float],\n",
    "        K: np.ndarray,\n",
    "        S: int,\n",
    "    ) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Follows the equation given in the hw 4 description.\n",
    "\n",
    "    Assumes the depth is not 0.\n",
    "\n",
    "    Parameters:\n",
    "        xy_and_depth(tuple): pixel coordinates and depth\n",
    "            of a detected corner\n",
    "        K(np.ndarray): 3x3 camera intrinsic matrix\n",
    "        S(int)\n",
    "\n",
    "    Returns: np.array - a row vector representing 3D worldspace\n",
    "        coordinates of the corner\n",
    "    \"\"\"\n",
    "    x, y, depth_val = xy_and_depth\n",
    "\n",
    "    image_coordinates_homogenous = np.array([x, y, 1]).T\n",
    "    worldspace_coordinates = (\n",
    "        (1 / S)  * depth_val * (linalg.inv(K) @ image_coordinates_homogenous)\n",
    "    )\n",
    "\n",
    "    assert worldspace_coordinates.shape == (3,)\n",
    "\n",
    "    return worldspace_coordinates.reshape(1, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply this function across all the corners, in all the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_3d_points = dict()\n",
    "\n",
    "for image_index, image_path in enumerate(rgb_paths):\n",
    "\n",
    "     # per image, convert appropiate corners to 3d points\n",
    "     _, top_k_points = path_to_image_and_corners[image_path]\n",
    "     top_k_points = top_k_points.astype(int)\n",
    "     corresponding_depthmap = depthmaps[image_index]\n",
    "     depth_vals = corresponding_depthmap[\n",
    "          top_k_points[:, 0],\n",
    "          top_k_points[:, 1]\n",
    "     ].reshape(-1, 1)\n",
    "\n",
    "     world_space_coordinates_per_image = list()\n",
    "\n",
    "     for depth_val_index, depth_val in enumerate(depth_vals):\n",
    "          if depth_val != 0:\n",
    "               x, y = (\n",
    "                    top_k_points[depth_val_index, 1],\n",
    "                    top_k_points[depth_val_index, 0],\n",
    "               )\n",
    "               world_space_coord = convert_1_corner_to_3d(\n",
    "                    (x, y, depth_val),\n",
    "                    K,\n",
    "                    S,\n",
    "               )\n",
    "               world_space_coordinates_per_image.append(world_space_coord)\n",
    "\n",
    "     path_to_3d_points[image_path] = np.asarray(world_space_coordinates_per_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Corner Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply a Rank Transform to All of the Images\n",
    "\n",
    "Using 5x5 windows here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.rank_transform import RankTransform2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "illumination_images_rank_transformed = [\n",
    "    RankTransform2D.transform(image, filter_side_length=5)\n",
    "    for image in illumination_images\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Wrangling\n",
    "\n",
    "I have a helper function to compute the putative correspondences. But it first requires us to associate the locations of the corners with their values in the corresponding rank transformed image (which I'm also referring to as its \"descriptor\" below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptors_of_all_images = dict()\n",
    "\n",
    "for image_index, image_path in enumerate(rgb_paths):\n",
    "\n",
    "    # per image, associate each corner location to its val in the rank transformed image\n",
    "    _, top_k_points = path_to_image_and_corners[image_path]\n",
    "    top_k_points = top_k_points.astype(int)\n",
    "    corresponding_descriptor_image = illumination_images_rank_transformed[image_index]\n",
    "\n",
    "    descriptors_per_image = list()\n",
    "\n",
    "    for index_corner in np.arange(top_k_points.shape[0]):\n",
    "        y, x = top_k_points[index_corner, :2]\n",
    "        descriptors_per_image.append([\n",
    "            y, x, np.array([corresponding_descriptor_image[y, x]])\n",
    "        ])\n",
    "\n",
    "    descriptors_of_all_images[image_path] = descriptors_per_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correspondence Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.ops import SimilarityMeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_DESIRED_MATCHES = 10  # determined by the hw 4 description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Images 2 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "image2_corner_descriptors = descriptors_of_all_images[rgb_paths[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "image1_corner_descriptors = descriptors_of_all_images[rgb_paths[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_2_to_1_top_corner_matches = HarrisCornerDetector.compute_feature_correspondences(\n",
    "    (image2_corner_descriptors, image1_corner_descriptors),\n",
    "    desired_num_similarities=NUM_DESIRED_MATCHES,\n",
    "    similarity_metric=SimilarityMeasure.SAD,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Images 2 and 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "image3_corner_descriptors = descriptors_of_all_images[rgb_paths[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_2_to_3_top_corner_matches = HarrisCornerDetector.compute_feature_correspondences(\n",
    "    (image2_corner_descriptors, image3_corner_descriptors),\n",
    "    desired_num_similarities=NUM_DESIRED_MATCHES,\n",
    "    similarity_metric=SimilarityMeasure.SAD,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
