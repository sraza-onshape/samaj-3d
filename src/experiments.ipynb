{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2\n",
    "#### Name: Syed Zain Raza\n",
    "#### CWID: 20011917"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: allow Jupyter to \"hot reload\" the Python modules I wrote, to avoid restarting the kernel after every change\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Teddy Stereo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Loading the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of ./teddy/teddyL.pgm: 375 x 450\n"
     ]
    }
   ],
   "source": [
    "left_img = ops.load_image(\n",
    "    \"./teddy/teddyL.pgm\",\n",
    "    return_grayscale=True,\n",
    "    return_array=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of ./teddy/teddyR.pgm: 375 x 450\n"
     ]
    }
   ],
   "source": [
    "right_img = ops.load_image(\n",
    "    \"./teddy/teddyR.pgm\",\n",
    "    return_grayscale=True,\n",
    "    return_array=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the Rank Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class RankTransform2D:\n",
    "    @staticmethod\n",
    "    def transform(\n",
    "        image: np.ndarray,\n",
    "        filter_side_length: int,\n",
    "        do_logging: bool = False,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Perform rank filtering on an image.\n",
    "\n",
    "        The goal is to produce a new image where each cell value\n",
    "        represents the \"rank\" of the corresponding pixel in the input\n",
    "        (i.e., the index of said pixel in a sorted list of itself &\n",
    "        the neighboring pixel values).\n",
    "\n",
    "        Parameters:\n",
    "            image(np.ndarray): in case its RGB, the transform will be\n",
    "                                per channel. Please pass the image in\n",
    "                                channels-last format.\n",
    "            filter_side_length(int): this is k. The size of each local neighborhood\n",
    "                              will be kxk. Please pass an odd value > 0.\n",
    "            do_logging(bool): enables print statements to display intermediate\n",
    "                              values during execution\n",
    "\n",
    "        Returns: np.ndarray: the transformed image\n",
    "        \"\"\"\n",
    "        ### HELPER(S)\n",
    "        def compute_rank(\n",
    "            channel: np.ndarray,\n",
    "            kernel: np.ndarray,\n",
    "            row_index: int,\n",
    "            col_index: int,\n",
    "        ) -> float:\n",
    "            \"\"\"\n",
    "            Computes the rank of 1 local window of the image.\n",
    "\n",
    "            Parameters:\n",
    "                channel(array-like): one of the channels of the input image\n",
    "                kernel(array-like): tells us the size of the window\n",
    "                row_index, col_index: int: the coordinates of the upper left corner\n",
    "                                            of the block of pixels being ranked\n",
    "\n",
    "            Returns: int: the rank of the center pixel of the window\n",
    "            \"\"\"\n",
    "            # A: define useful vars\n",
    "            kernel_h, kernel_w = kernel.shape\n",
    "            # B: get the block of pixels needed for the convolution\n",
    "            block_of_pixels = channel[\n",
    "                row_index : (kernel_h + row_index),\n",
    "                col_index : (kernel_w + col_index)\n",
    "            ]\n",
    "            # C: count the of # higher than the center\n",
    "            center_val = block_of_pixels[kernel_h // 2, kernel_w // 2]\n",
    "            if do_logging:\n",
    "                print(f\"I think that {center_val} is at the center of {block_of_pixels}\")\n",
    "            transformed_block = np.where(block_of_pixels < center_val, 1, 0)\n",
    "            if do_logging:\n",
    "                print(f\"Transformed block <{block_of_pixels}> into: <{transformed_block}>\")\n",
    "            return np.sum(transformed_block)\n",
    "\n",
    "        ### DRIVER\n",
    "        # data validation\n",
    "        assert isinstance(image, np.ndarray)\n",
    "        assert image.shape > (0, 0)\n",
    "        assert isinstance(filter_side_length, int)\n",
    "        assert filter_side_length > 0 and filter_side_length % 2 == 1\n",
    "\n",
    "        # make a copy of the img, padded - will be an intermediate repr\n",
    "        kernel = np.ones((filter_side_length, filter_side_length))\n",
    "        num_channels = -1\n",
    "        if len(image.shape) == 2:  # grayscale\n",
    "            num_channels = 1\n",
    "            padded_image, _, _ = ops.pad(\n",
    "                image, kernel, stride=1, padding_type=\"zero\"\n",
    "            )\n",
    "        elif len(image.shape) == 3:  # RGB\n",
    "            num_channels = image.shape[2]\n",
    "            channels = [\n",
    "                ops.pad(image[:, :, channel_index], kernel, stride=1, padding_type=\"zero\")[0]\n",
    "                for channel_index in range(num_channels)\n",
    "            ]\n",
    "            padded_image = np.dstack(channels)\n",
    "\n",
    "        # fill in the output\n",
    "        stride = 1\n",
    "        output_image = list()\n",
    "        for image_channel_index in np.arange(num_channels):\n",
    "            transformed_channel = list()\n",
    "            channel = (\n",
    "                padded_image[:, :, image_channel_index]\n",
    "                if num_channels > 1\n",
    "                else padded_image\n",
    "            )\n",
    "            kernel_h, _ = kernel.shape\n",
    "            # iterate over the rows and columns\n",
    "            starting_row_ndx = 0\n",
    "            while starting_row_ndx <= len(channel) - kernel_h:\n",
    "                # convolve the next row of this channel\n",
    "                next_channel_row = ops.slide_kernel_over_image(\n",
    "                    channel,\n",
    "                    kernel,\n",
    "                    starting_row_ndx,\n",
    "                    stride,\n",
    "                    apply=compute_rank,\n",
    "                )\n",
    "                # now, add the convolved row to the list\n",
    "                transformed_channel.append(next_channel_row)\n",
    "                # move to the next starting row for the filtering\n",
    "                starting_row_ndx += stride\n",
    "            output_image.append(transformed_channel)\n",
    "        # stack the channels, and return\n",
    "        return np.dstack(output_image).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: Rank Transform the Left View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_left = RankTransform2D.transform(\n",
    "    left_img, filter_side_length=5, do_logging=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check to make sure the rank transform produced a new image of the same dimensions as the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_left.shape == left_img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Computing a Disparity Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "import functools\n",
    "from typing import Literal, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from util import ops\n",
    "from util.ops import SimilarityMeasure\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SimpleStereoDisparityMap:\n",
    "    \"\"\"\n",
    "    Computes a disparity map between two images based on the\n",
    "    assumption that they are:\n",
    "        1) of the same shape,\n",
    "\n",
    "        2) grayscale,\n",
    "\n",
    "        3) and already rectified.\n",
    "    \"\"\"\n",
    "    left_image: np.ndarray\n",
    "    right_image: np.ndarray\n",
    "    stride: int = 1\n",
    "    padding_type: Union[Literal[\"zero\"], Literal[\"repeat\"]] = \"zero\"\n",
    "\n",
    "    def compute(\n",
    "        self: SimpleStereoDisparityMap,\n",
    "        similarity_measure: SimilarityMeasure,\n",
    "        rank_transform_filter_side_length: int,\n",
    "        window_size: int,\n",
    "        do_logging: bool = False,\n",
    "        max_disparity_level: int = 63\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute pixel-wise disparity by searching in the second image\n",
    "        the most similar patch along the same row in the first image.\n",
    "        \"\"\"\n",
    "        ### HELPER(S)\n",
    "        def _fetch_image_patches(\n",
    "            ptr: np.ndarray,  # aka the padded, rank transformed repr of the right image\n",
    "            left_pixel_block: np.ndarray,\n",
    "            starting_row_index: int,\n",
    "            ptr_starting_col_index: int,\n",
    "        ) -> int:\n",
    "            \"\"\"\n",
    "            TODO[add docstring]\n",
    "\n",
    "            Args:\n",
    "                channel: 2D array - one of the channels of the input image\n",
    "                kernel: 2D array representing the parameters to use\n",
    "                starting_row_index, ptr_starting_col_index: int: the coordinates of the upper left corner\n",
    "                                    of the block of pixels being convolved\n",
    "\n",
    "            Returns: int: TODO\n",
    "            \"\"\"\n",
    "            # form a list of tuple(patch, center_pixel_column)\n",
    "            kernel_h, kernel_w = left_pixel_block.shape\n",
    "            right_image_starting_col_indices = np.arange(\n",
    "                ptr_starting_col_index,\n",
    "                max(-1, ptr_starting_col_index - max_disparity_level),\n",
    "                -1 * self.stride\n",
    "            )\n",
    "            if do_logging:\n",
    "                print(\n",
    "                    f\"Starting to fetch patches starting at indices: {right_image_starting_col_indices}.\"\n",
    "                )\n",
    "            patches = np.zeros((\n",
    "                right_image_starting_col_indices.shape[0],\n",
    "                kernel_h * kernel_w\n",
    "            ))\n",
    "            center_col_indices = np.zeros(right_image_starting_col_indices.shape[0])\n",
    "            iter_num = 0\n",
    "            for right_image_starting_col_ndx in right_image_starting_col_indices:\n",
    "                patch = ptr[\n",
    "                    starting_row_index : starting_row_index + kernel_h,\n",
    "                    right_image_starting_col_ndx : right_image_starting_col_ndx + kernel_w,\n",
    "                ]\n",
    "                patches[iter_num, :] = patch.reshape(kernel_h*kernel_w)\n",
    "\n",
    "                right_image_center_col_index = right_image_starting_col_ndx + (kernel_w // 2)\n",
    "                center_col_indices[iter_num] = right_image_center_col_index\n",
    "\n",
    "                iter_num += 1\n",
    "            # for each patch in the row of the right image, compute the SAD with the same block from the first\n",
    "            compute_sad = functools.partial(\n",
    "                ops.compute_similarity,\n",
    "                mode=similarity_measure,\n",
    "                arr1=left_pixel_block\n",
    "            )\n",
    "            # print(f\"Patches and center col indices has shape: {patches.shape, center_col_indices.shape}\")\n",
    "            if patches.shape[0] == 0:\n",
    "                # print(f\"Patches: {patches}\")\n",
    "                if do_logging:\n",
    "                    print(\"No patches! exiting early with same col, this right image col has no corr with the left image\")\n",
    "                return ptr_starting_col_index\n",
    "            # TODO[Zain]: optimize in the future\n",
    "            hypothesis_similarities = np.apply_along_axis(\n",
    "                lambda arr2: compute_sad(arr2=arr2.reshape((kernel_h, kernel_w))),\n",
    "                axis=1,\n",
    "                arr=patches,\n",
    "            )\n",
    "            # hypothesis_similarities = list()\n",
    "            # for patch_index in np.arange(patches.shape[0]):\n",
    "            #     patch = patches[patch_index, :].reshape((kernel_h, kernel_w))\n",
    "            #     hypothesis_similarities.append(\n",
    "            #         compute_sad(arr2=patch)\n",
    "            #     )\n",
    "            # hypothesis_similarities = np.array(hypothesis_similarities)\n",
    "            assert hypothesis_similarities.shape == center_col_indices.shape, (\n",
    "                f\"Expected hypothesis_similarities to have a shape of: {center_col_indices.shape}, \",\n",
    "                f\"but actual shape is: {hypothesis_similarities.shape}\",\n",
    "            )\n",
    "            # at the end, choose the patch of the right image with the most similarity\n",
    "            best_center_col_hypothesis = center_col_indices[np.argmin(hypothesis_similarities)]\n",
    "            return best_center_col_hypothesis\n",
    "\n",
    "        ### DRIVER\n",
    "        # data validations\n",
    "        assert self.left_image.shape == self.right_image.shape, (\n",
    "            f\"Image shape mismatch between left <{self.left_image.shape}>\"\n",
    "            f\" and right <{self.right_image.shape}> views.\"\n",
    "        )\n",
    "        assert len(self.left_image.shape) == 2, (\n",
    "            f\"Expected a 2D array representing a grayscale image, \"\n",
    "            f\"actual number of channels is: {self.left_image.shape[2]}.\"\n",
    "        )\n",
    "        # rank transform both images\n",
    "        transformed_left = RankTransform2D.transform(self.left_image, rank_transform_filter_side_length)\n",
    "        transformed_right = RankTransform2D.transform(self.right_image, rank_transform_filter_side_length)\n",
    "        # create a square kernel of all 1's using the given window size\n",
    "        kernel = np.ones((window_size, window_size))\n",
    "        # pad both images\n",
    "        padded_transformed_left, _, _ = ops.pad(\n",
    "            transformed_left,\n",
    "            kernel,\n",
    "            stride=self.stride,\n",
    "            padding_type=self.padding_type\n",
    "        )\n",
    "        ptl = padded_transformed_left\n",
    "        padded_transformed_right, _, _ = ops.pad(\n",
    "            transformed_right,\n",
    "            kernel,\n",
    "            stride=self.stride,\n",
    "            padding_type=self.padding_type\n",
    "        )\n",
    "        ptr = padded_transformed_right\n",
    "        assert (\n",
    "            ptl.shape == ptr.shape\n",
    "        ), f\"Shape mismatch after padding: {ptl.shape} != {ptr.shape}\"\n",
    "        # create the output image\n",
    "        output_image = list()\n",
    "        kernel_h, kernel_w = kernel.shape\n",
    "        # for every row in the first image\n",
    "        for starting_row_ndx in np.arange(0, ptl.shape[0] - kernel_h, self.stride):\n",
    "            output_image_row = list()\n",
    "            for left_image_starting_col_ndx in np.arange(0, ptl.shape[1] - kernel_w, self.stride):\n",
    "                left_pixel_block = padded_transformed_left[\n",
    "                    starting_row_ndx : starting_row_ndx + kernel_h,\n",
    "                    left_image_starting_col_ndx : left_image_starting_col_ndx + kernel_w,\n",
    "                ]\n",
    "                left_block_center_pixel_index = starting_row_ndx + (kernel_h // 2)\n",
    "                # slide the kernel over the same row in the second image\n",
    "                best_center_col_hypothesis = ops.slide_kernel_over_image(\n",
    "                    padded_transformed_right,\n",
    "                    kernel=left_pixel_block,\n",
    "                    row_index=starting_row_ndx,\n",
    "                    stride=self.stride,\n",
    "                    apply=_fetch_image_patches,\n",
    "                )\n",
    "                best_center_col_hypothesis = np.array(best_center_col_hypothesis)\n",
    "                if do_logging:\n",
    "                    print(f\"best_center_col_hypothesis: {best_center_col_hypothesis}\")\n",
    "                # assert (\n",
    "                #     left_image_starting_col_ndx\n",
    "                #     <= np.all(best_center_col_hypothesis)\n",
    "                #     < ptr.shape[1]\n",
    "                # ), (\n",
    "                #     f\"Impossible location of right correspondence is column index {best_center_col_hypothesis} \",\n",
    "                #     f\"is not between [{left_image_starting_col_ndx}, {ptr.shape[1]}).\",\n",
    "                # )\n",
    "                # if best_center_col_hypothesis.max() >= ptr.shape[1]:\n",
    "                #     too_high_indices = np.where(best_center_col_hypothesis >= ptr.shape[1], 1, 0)\n",
    "                #     too_high = best_center_col_hypothesis[too_high_indices]\n",
    "                #     print(f\"Impossible center col index {too_high} is >= {ptr.shape[1]}\")\n",
    "                # if best_center_col_hypothesis.min() < 0:\n",
    "                #     too_low_indices = np.where(best_center_col_hypothesis == 0, 1, 0)\n",
    "                #     too_low = best_center_col_hypothesis[too_low_indices]\n",
    "                #     print(f\"Impossible center col index {too_low} is < {0}\")\n",
    "                disparity = np.abs(left_block_center_pixel_index - best_center_col_hypothesis)\n",
    "                disparity = np.where(disparity > 63, 0, disparity)\n",
    "                if do_logging:\n",
    "                    print(f\"Upper left corner of patch has coords: ({starting_row_ndx}, {left_image_starting_col_ndx}).\")\n",
    "                    print(f\"Window size is {window_size}, so center is at col index: {left_block_center_pixel_index}\")\n",
    "                assert np.min(disparity) >= 0, f\"disparity of {disparity} is too low\"\n",
    "                assert np.max(disparity) <= 63, f\"disparity of {disparity} is too high\"\n",
    "                output_image_row.append(disparity)\n",
    "            output_image.append(output_image_row)\n",
    "        # final checks\n",
    "        output = np.array(output_image)\n",
    "        assert output.shape == self.left_image.shape, f\"Shape mismatch {output.shape} != {self.left_image.shape}\"\n",
    "        return output\n",
    "\n",
    "    @classmethod\n",
    "    def compute_and_visualize(\n",
    "        cls: SimpleStereoDisparityMap,\n",
    "        image1: np.ndarray,\n",
    "        image2: np.ndarray,\n",
    "        similarity_measure: SimilarityMeasure,\n",
    "        rank_transform_filter_side_length: int,\n",
    "        window_size: int,\n",
    "        scene_name: str,\n",
    "        do_logging: bool = False,\n",
    "        stride: int = 1,\n",
    "        padding_type: Union[Literal[\"zero\"], Literal[\"repeat\"]] = \"zero\",\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Convenience wrapper that both computes and plots\n",
    "        1 disparity map for a pair of stereo images.\n",
    "        TODO\n",
    "        \"\"\"\n",
    "        mapper = cls(\n",
    "            left_image=image1,\n",
    "            right_image=image2,\n",
    "            stride=stride,\n",
    "            padding_type=padding_type,\n",
    "        )\n",
    "        disparity_map = mapper.compute(\n",
    "            similarity_measure=similarity_measure,\n",
    "            rank_transform_filter_side_length=rank_transform_filter_side_length,\n",
    "            window_size=window_size,\n",
    "            do_logging=do_logging,\n",
    "        )\n",
    "        plt.imshow(disparity_map)\n",
    "        plt.title(f\"Left and Right View Disparity for \\\"{scene_name}\\\"\")\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: compare both the implementations of 3x3 window and 15x15 against the same GT map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the `SimpleStereoDisparityMap` with a 3x3 Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mSimpleStereoDisparityMap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_and_visualize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mleft_img\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mright_img\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mSimilarityMeasure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSAD\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrank_transform_filter_side_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscene_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTeddy Bear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_logging\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzero\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 230\u001b[0m, in \u001b[0;36mSimpleStereoDisparityMap.compute_and_visualize\u001b[0;34m(cls, image1, image2, similarity_measure, rank_transform_filter_side_length, window_size, scene_name, do_logging, stride, padding_type)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;124;03mConvenience wrapper that both computes and plots\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;124;03m1 disparity map for a pair of stereo images.\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;124;03mTODO\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    224\u001b[0m mapper \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\n\u001b[1;32m    225\u001b[0m     left_image\u001b[38;5;241m=\u001b[39mimage1,\n\u001b[1;32m    226\u001b[0m     right_image\u001b[38;5;241m=\u001b[39mimage2,\n\u001b[1;32m    227\u001b[0m     stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[1;32m    228\u001b[0m     padding_type\u001b[38;5;241m=\u001b[39mpadding_type,\n\u001b[1;32m    229\u001b[0m )\n\u001b[0;32m--> 230\u001b[0m disparity_map \u001b[38;5;241m=\u001b[39m \u001b[43mmapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43msimilarity_measure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msimilarity_measure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrank_transform_filter_side_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrank_transform_filter_side_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_logging\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_logging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(disparity_map)\n\u001b[1;32m    237\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLeft and Right View Disparity for \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mscene_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 166\u001b[0m, in \u001b[0;36mSimpleStereoDisparityMap.compute\u001b[0;34m(self, similarity_measure, rank_transform_filter_side_length, window_size, do_logging, max_disparity_level)\u001b[0m\n\u001b[1;32m    164\u001b[0m left_block_center_pixel_index \u001b[38;5;241m=\u001b[39m starting_row_ndx \u001b[38;5;241m+\u001b[39m (kernel_h \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# slide the kernel over the same row in the second image\u001b[39;00m\n\u001b[0;32m--> 166\u001b[0m best_center_col_hypothesis \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mslide_kernel_over_image\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadded_transformed_right\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkernel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_pixel_block\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrow_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstarting_row_ndx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapply\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fetch_image_patches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m best_center_col_hypothesis \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(best_center_col_hypothesis)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_logging:\n",
      "File \u001b[0;32m~/repos/CS-532-3D-Computer-Vision/src/util/ops.py:225\u001b[0m, in \u001b[0;36mslide_kernel_over_image\u001b[0;34m(channel, kernel, row_index, stride, apply)\u001b[0m\n\u001b[1;32m    222\u001b[0m starting_col_ndx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m starting_col_ndx \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(channel[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m-\u001b[39m kernel_w:\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;66;03m# compute the convolution\u001b[39;00m\n\u001b[0;32m--> 225\u001b[0m     conv_block_of_pixels \u001b[38;5;241m=\u001b[39m \u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchannel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstarting_col_ndx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;66;03m# add it to the output\u001b[39;00m\n\u001b[1;32m    227\u001b[0m     conv_channel_row\u001b[38;5;241m.\u001b[39mappend(conv_block_of_pixels)\n",
      "Cell \u001b[0;32mIn[11], line 86\u001b[0m, in \u001b[0;36mSimpleStereoDisparityMap.compute.<locals>._fetch_image_patches\u001b[0;34m(ptr, left_pixel_block, starting_row_index, ptr_starting_col_index)\u001b[0m\n\u001b[1;32m     83\u001b[0m     right_image_center_col_index \u001b[38;5;241m=\u001b[39m right_image_starting_col_ndx \u001b[38;5;241m+\u001b[39m (kernel_w \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     84\u001b[0m     center_col_indices[iter_num] \u001b[38;5;241m=\u001b[39m right_image_center_col_index\n\u001b[0;32m---> 86\u001b[0m     iter_num \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# for each patch in the row of the right image, compute the SAD with the same block from the first\u001b[39;00m\n\u001b[1;32m     88\u001b[0m compute_sad \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[1;32m     89\u001b[0m     ops\u001b[38;5;241m.\u001b[39mcompute_similarity,\n\u001b[1;32m     90\u001b[0m     mode\u001b[38;5;241m=\u001b[39msimilarity_measure,\n\u001b[1;32m     91\u001b[0m     arr1\u001b[38;5;241m=\u001b[39mleft_pixel_block\n\u001b[1;32m     92\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "SimpleStereoDisparityMap.compute_and_visualize(\n",
    "    left_img,\n",
    "    right_img,\n",
    "    SimilarityMeasure.SAD,\n",
    "    rank_transform_filter_side_length=5,\n",
    "    window_size=3,\n",
    "    scene_name=\"Teddy Bear\",\n",
    "    do_logging=False,\n",
    "    stride=1,\n",
    "    padding_type=\"zero\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
