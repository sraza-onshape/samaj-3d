{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2\n",
    "#### Name: Syed Zain Raza\n",
    "#### CWID: 20011917"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: allow Jupyter to \"hot reload\" the Python modules I wrote, to avoid restarting the kernel after every change\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Teddy Stereo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Loading the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of ./teddy/teddyL.pgm: 375 x 450\n"
     ]
    }
   ],
   "source": [
    "left_img = ops.load_image(\n",
    "    \"./teddy/teddyL.pgm\",\n",
    "    return_grayscale=True,\n",
    "    return_array=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of ./teddy/teddyR.pgm: 375 x 450\n"
     ]
    }
   ],
   "source": [
    "right_img = ops.load_image(\n",
    "    \"./teddy/teddyR.pgm\",\n",
    "    return_grayscale=True,\n",
    "    return_array=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the Rank Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class RankTransform2D:\n",
    "    @staticmethod\n",
    "    def transform(\n",
    "        image: np.ndarray,\n",
    "        filter_side_length: int,\n",
    "        do_logging: bool = False,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Perform rank filtering on an image.\n",
    "\n",
    "        The goal is to produce a new image where each cell value\n",
    "        represents the \"rank\" of the corresponding pixel in the input\n",
    "        (i.e., the index of said pixel in a sorted list of itself &\n",
    "        the neighboring pixel values).\n",
    "\n",
    "        Parameters:\n",
    "            image(np.ndarray): in case its RGB, the transform will be\n",
    "                                per channel. Please pass the image in\n",
    "                                channels-last format.\n",
    "            filter_side_length(int): this is k. The size of each local neighborhood\n",
    "                              will be kxk. Please pass an odd value > 0.\n",
    "        \n",
    "        Returns: np.ndarray: the transformed image\n",
    "        \"\"\"\n",
    "        ### HELPER(S)\n",
    "        def compute_rank(\n",
    "            channel: np.ndarray,\n",
    "            kernel: np.ndarray,\n",
    "            row_index: int,\n",
    "            col_index: int,\n",
    "        ) -> float:\n",
    "            \"\"\"\n",
    "            Computes the rank of 1 local window of the image.\n",
    "\n",
    "            Parameters:\n",
    "                channel(array-like): one of the channels of the input image\n",
    "                kernel(array-like): tells us the size of the window \n",
    "                row_index, col_index: int: the coordinates of the upper left corner\n",
    "                                            of the block of pixels being ranked\n",
    "\n",
    "            Returns: int: the rank of the center pixel of the window\n",
    "            \"\"\"\n",
    "            # A: define useful vars\n",
    "            kernel_h, kernel_w = kernel.shape\n",
    "            # B: get the block of pixels needed for the convolution\n",
    "            block_of_pixels = channel[\n",
    "                row_index : (kernel_h + row_index),\n",
    "                col_index : (kernel_w + col_index)\n",
    "            ]\n",
    "            # C: count the of # higher than the center\n",
    "            center_val = block_of_pixels[kernel_h // 2, kernel_w // 2]\n",
    "            if do_logging:\n",
    "                print(f\"I think that {center_val} is at the center of {block_of_pixels}\")\n",
    "            transformed_block = np.where(block_of_pixels < center_val, 1, 0)\n",
    "            if do_logging:\n",
    "                print(f\"Transformed block <{block_of_pixels}> into: <{transformed_block}>\")\n",
    "            return np.sum(transformed_block)\n",
    "\n",
    "        ### DRIVER\n",
    "        # data validation\n",
    "        assert isinstance(image, np.ndarray) \n",
    "        assert image.shape > (0, 0)\n",
    "        assert isinstance(filter_side_length, int)\n",
    "        assert filter_side_length > 0 and filter_side_length % 2 == 1\n",
    "\n",
    "        # make a copy of the img, padded - will be an intermediate repr\n",
    "        kernel = np.ones((filter_side_length, filter_side_length))\n",
    "        num_channels = -1\n",
    "        if len(image.shape) == 2:  # grayscale\n",
    "            num_channels = 1\n",
    "            padded_image, _, _ = ops.pad(\n",
    "                image, kernel, stride=1, padding_type=\"zero\"\n",
    "            )\n",
    "        elif len(image.shape) == 3:  # RGB\n",
    "            num_channels = image.shape[2]\n",
    "            channels = [\n",
    "                ops.pad(image[:, :, channel_index], kernel, stride=1, padding_type=\"zero\")[0]\n",
    "                for channel_index in range(num_channels)\n",
    "            ]\n",
    "            padded_image = np.dstack(channels)\n",
    "\n",
    "        # fill in the output\n",
    "        stride = 1\n",
    "        output_image = list()\n",
    "        for image_channel_index in np.arange(num_channels):\n",
    "            transformed_channel = list()\n",
    "            channel = (\n",
    "                padded_image[:, :, image_channel_index]\n",
    "                if num_channels > 1\n",
    "                else padded_image\n",
    "            )\n",
    "            kernel_h, _ = kernel.shape\n",
    "            # iterate over the rows and columns\n",
    "            starting_row_ndx = 0\n",
    "            while starting_row_ndx <= len(channel) - kernel_h:\n",
    "                # convolve the next row of this channel\n",
    "                next_channel_row = ops.slide_kernel_over_image(\n",
    "                    channel,\n",
    "                    kernel,\n",
    "                    starting_row_ndx,\n",
    "                    stride,\n",
    "                    apply=compute_rank,\n",
    "                )\n",
    "                # now, add the convolved row to the list\n",
    "                transformed_channel.append(next_channel_row)\n",
    "                # move to the next starting row for the filtering\n",
    "                starting_row_ndx += stride\n",
    "            output_image.append(transformed_channel)\n",
    "        # stack the channels, and return\n",
    "        return np.dstack(output_image).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rank Transforming the Left View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_left = RankTransform2D.transform(\n",
    "    left_img, filter_side_length=5, do_logging=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check to make sure the rank transform produced a new image of the same dimensions as the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_left.shape == left_img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Computing a Disparity Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "import functools\n",
    "from typing import Literal, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from util import ops\n",
    "from util.ops import SimilarityMeasure\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SimpleStereoDisparityMap:\n",
    "    \"\"\"\n",
    "    Computes a disparity map between two images based on the\n",
    "    assumption that they are:\n",
    "        1) of the same shape, \n",
    "        2) grayscale,\n",
    "        3) and already rectified.\n",
    "    \"\"\"\n",
    "    left_image: np.ndarray\n",
    "    right_image: np.ndarray\n",
    "    stride: int = 1\n",
    "    padding_type: Union[Literal[\"zero\"], Literal[\"repeat\"]] = \"zero\"\n",
    "\n",
    "    def compute(\n",
    "            self: SimpleStereoDisparityMap,\n",
    "            similarity_measure: SimilarityMeasure,\n",
    "            rank_transform_filter_side_length: int,\n",
    "            window_size: int,\n",
    "            do_logging: bool = False,\n",
    "        ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute pixel-wise disparity by searching in the second image \n",
    "        the most similar patch along the same row in the first image.\n",
    "        \"\"\"\n",
    "        ### HELPER(S)\n",
    "        def _fetch_image_patches(\n",
    "            ptr: np.ndarray,  # aka, the padded, rank transformed repr of the right\n",
    "            left_pixel_block: np.ndarray,\n",
    "            row_index: int,\n",
    "            ptr_starting_col_index: int,\n",
    "        ) -> int:\n",
    "            \"\"\"\n",
    "            TODO\n",
    "\n",
    "            Args:\n",
    "                channel: 2D array - one of the channels of the input image\n",
    "                kernel: 2D array representing the parameters to use\n",
    "                row_index, col_index: int: the coordinates of the upper left corner\n",
    "                                    of the block of pixels being convolved\n",
    "\n",
    "            Returns: int: TODO\n",
    "            \"\"\"\n",
    "            # form a list of tuple(patch, center_pixel_column)\n",
    "            kernel_h, kernel_w = left_pixel_block.shape\n",
    "            right_image_starting_col_indices = np.arange(\n",
    "                ptr_starting_col_index, ptr.shape[1] - kernel_w, self.stride\n",
    "            )\n",
    "            patches = np.zeros((\n",
    "                right_image_starting_col_indices.shape[0],\n",
    "                kernel_h, kernel_w),\n",
    "            )\n",
    "            center_col_indices = np.zeros(right_image_starting_col_indices.shape[0])\n",
    "            for right_image_starting_col_ndx in right_image_starting_col_indices:\n",
    "                patch = ptr[\n",
    "                    row_index:row_index + kernel_h,\n",
    "                    right_image_starting_col_ndx:right_image_starting_col_ndx + kernel_w\n",
    "                ]\n",
    "                patches[right_image_starting_col_ndx, :, :] = patch\n",
    "\n",
    "                right_image_center_col_index = right_image_starting_col_ndx + (kernel_w // 2)\n",
    "                center_col_indices[right_image_starting_col_ndx] = right_image_center_col_index\n",
    "            # for each patch in the row of the right image, compute the SAD with the same block from the first\n",
    "            compute_sad = functools.partial(\n",
    "                ops.compute_similarity,\n",
    "                mode=similarity_measure,\n",
    "                arr1=left_pixel_block\n",
    "            )\n",
    "            hypothesis_similarities = np.apply_along_axis(\n",
    "                compute_sad,\n",
    "                axis=0,\n",
    "                arr=patches,\n",
    "            )\n",
    "            # at the end, choose the patch of the right image with the most similarity\n",
    "            best_center_col_hypothesis = center_col_indices[np.argmin(hypothesis_similarities)]\n",
    "            return best_center_col_hypothesis\n",
    "\n",
    "        ### DRIVER\n",
    "        # data validations\n",
    "        assert self.left_image.shape == self.right_image.shape, (\n",
    "            f\"Image shape mismatch between left <{self.left_image.shape}>\"\n",
    "            f\" and right <{self.right_image.shape}> views.\"\n",
    "        )\n",
    "        assert len(self.left_image) == 2, (\n",
    "            f\"Expected a 2D array representing a grayscale image, \"\n",
    "            f\"actual number of channels is: {self.left_image.shape[2]}.\"\n",
    "        )\n",
    "        # rank transform both images\n",
    "        transformed_left = RankTransform2D.transform(self.left_image, rank_transform_filter_side_length)\n",
    "        transformed_right = RankTransform2D.transform(self.right_image, rank_transform_filter_side_length)\n",
    "        # create a square kernel of all 1's using the given window size\n",
    "        kernel = np.ones((window_size, window_size))\n",
    "        # pad both images\n",
    "        padded_transformed_left = ptl = ops.pad(\n",
    "            transformed_left,\n",
    "            kernel,\n",
    "            stride=self.stride,\n",
    "            padding_type=self.padding_type\n",
    "        )\n",
    "        padded_transformed_right = ops.pad(\n",
    "            transformed_right,\n",
    "            kernel,\n",
    "            stride=self.stride,\n",
    "            padding_type=self.padding_type\n",
    "        )\n",
    "        # create the output image\n",
    "        output_image = list()\n",
    "        kernel_h, kernel_w = kernel.shape\n",
    "        # for every row in the first image\n",
    "        for starting_row_ndx in np.arange(0, ptl.shape[0] - kernel_h, self.stride):\n",
    "            output_image_row = list()\n",
    "            for left_image_starting_col_ndx in np.arange(0, ptl.shape[1] - kernel_w, self.stride):\n",
    "                left_pixel_block = padded_transformed_left[\n",
    "                    starting_row_ndx : starting_row_ndx + kernel_h,\n",
    "                    left_image_starting_col_ndx : left_image_starting_col_ndx + kernel_w,\n",
    "                ]\n",
    "                left_block_center_pixel_index = starting_row_ndx + (kernel_h // 2)\n",
    "                # slide the kernel over the same row in the second image\n",
    "                best_center_col_hypothesis = ops.slide_kernel_over_image(\n",
    "                    padded_transformed_right,\n",
    "                    kernel=left_pixel_block,\n",
    "                    row_index=starting_row_ndx,\n",
    "                    stride=self.stride,\n",
    "                    apply=_fetch_image_patches,\n",
    "                )\n",
    "                disparity = np.abs(best_center_col_hypothesis - left_block_center_pixel_index)\n",
    "                output_image_row.append(disparity)\n",
    "            output_image.append(output_image_row)\n",
    "        # all done!\n",
    "        output = np.array(output_image)\n",
    "        assert output.shape == self.left_image.shape, f\"Shape mismatch {output.shape} != {self.left_image.shape}\"\n",
    "        assert output.min() >= 0, f\"min of output is too low\"\n",
    "        assert output.max() <= 63, f\"max of output is too high\"\n",
    "        return output\n",
    "\n",
    "    @classmethod\n",
    "    def compute_and_visualize(\n",
    "        cls: SimpleStereoDisparityMap,\n",
    "        image1: np.ndarray,\n",
    "        image2: np.ndarray,\n",
    "        similarity_measure: SimilarityMeasure,\n",
    "        rank_transform_filter_side_length: int,\n",
    "        window_size: int,\n",
    "        scene_name: str,\n",
    "        do_logging: bool = False,\n",
    "        stride: int = 1,\n",
    "        padding_type: Union[Literal[\"zero\"], Literal[\"repeat\"]] = \"zero\",\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Convenience wrapper that both computes and plots\n",
    "        1 disparity map for a pair of stereo images.\n",
    "        TODO\n",
    "        \"\"\"\n",
    "        mapper = cls(\n",
    "            left_image=image1,\n",
    "            right_image=image2,\n",
    "            stride=stride,\n",
    "            padding_type=padding_type,\n",
    "        )\n",
    "        disparity_map = mapper.compute(\n",
    "            similarity_measure=similarity_measure,\n",
    "            rank_transform_filter_side_length=rank_transform_filter_side_length,\n",
    "            window_size=window_size,\n",
    "            do_logging=do_logging,\n",
    "        )\n",
    "        plt.imshow(disparity_map)\n",
    "        plt.title(f\"Left and Right View Disparity for \\\"{scene_name}\\\"\")\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: compare both the implementations of 3x3 window and 15x15 against the same GT map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
