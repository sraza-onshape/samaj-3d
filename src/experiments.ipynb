{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2\n",
    "#### Name: Syed Zain Raza\n",
    "#### CWID: 20011917"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# optional: allow Jupyter to \"hot reload\" the Python modules I wrote, to avoid restarting the kernel after every change\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Teddy Stereo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Loading the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of ./teddy/teddyL.pgm: 375 x 450\n"
     ]
    }
   ],
   "source": [
    "left_img = ops.load_image(\n",
    "    \"./teddy/teddyL.pgm\",\n",
    "    return_grayscale=True,\n",
    "    return_array=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of ./teddy/teddyR.pgm: 375 x 450\n"
     ]
    }
   ],
   "source": [
    "right_img = ops.load_image(\n",
    "    \"./teddy/teddyR.pgm\",\n",
    "    return_grayscale=True,\n",
    "    return_array=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the Rank Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class RankTransform2D:\n",
    "    @staticmethod\n",
    "    def transform(\n",
    "        image: np.ndarray,\n",
    "        filter_size: int,\n",
    "        do_logging: bool = False,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Perform a rank filtering operation on an image.\n",
    "\n",
    "        The goal is to produce a new image where each cell value\n",
    "        represents the \"rank\" of the corresponding pixel in the input\n",
    "        (i.e., the index of said pixel in a sorted list of itself &\n",
    "        the neighboring pixel values).\n",
    "\n",
    "        Parameters:\n",
    "            image(np.ndarray): in case its RGB, the transform will be\n",
    "                                per channel. Please pass the image in\n",
    "                                channels-last format.\n",
    "            filter_size(int): this is k. The size of each local neighborhood\n",
    "                              will be kxk. Please pass an odd value > 0.\n",
    "        \n",
    "        Returns: np.ndarray: the transformed image\n",
    "        \"\"\"\n",
    "        ### HELPER(S)\n",
    "        def compute_rank(\n",
    "            channel: np.ndarray,\n",
    "            kernel: np.ndarray,\n",
    "            row_index: int,\n",
    "            col_index: int,\n",
    "        ) -> float:\n",
    "            \"\"\"\n",
    "            Computes the rank of 1 local window of the image.\n",
    "\n",
    "            Parameters:\n",
    "                channel(array-like): one of the channels of the input image\n",
    "                kernel(array-like): tells us the size of the window \n",
    "                row_index, col_index: int: the coordinates of the upper left corner\n",
    "                                            of the block of pixels being ranked\n",
    "\n",
    "            Returns: int: the rank of the center pixel of the window\n",
    "            \"\"\"\n",
    "            # A: define useful vars\n",
    "            kernel_h, kernel_w = kernel.shape\n",
    "            # B: get the block of pixels needed for the convolution\n",
    "            block_of_pixels = channel[\n",
    "                row_index : (kernel_h + row_index),\n",
    "                col_index : (kernel_w + col_index)\n",
    "            ]\n",
    "            # C: count the of # higher than the center\n",
    "            center_val = block_of_pixels[kernel_h // 2, kernel_w // 2]\n",
    "            if do_logging:\n",
    "                print(f\"I think that {center_val} is at the center of {block_of_pixels}\")\n",
    "            transformed_block = np.where(block_of_pixels < center_val, 1, 0)\n",
    "            if do_logging:\n",
    "                print(f\"Transformed bloc <{block_of_pixels}> into: <{transformed_block}>\")\n",
    "            return np.sum(transformed_block)\n",
    "\n",
    "        ### DRIVER\n",
    "        # data validation\n",
    "        assert isinstance(image, np.ndarray) \n",
    "        assert image.shape > (0, 0)\n",
    "        assert isinstance(filter_size, int)\n",
    "        assert filter_size > 0 and filter_size % 2 == 1\n",
    "\n",
    "        # make a copy of the img, padded - will be an intermediate repr\n",
    "        kernel = np.ones((filter_size, filter_size))\n",
    "        num_channels = -1\n",
    "        if len(image.shape) == 2:  # grayscale\n",
    "            num_channels = 1\n",
    "            padded_image, _, _ = ops.pad(\n",
    "                image, kernel, stride=1, padding_type=\"zero\"\n",
    "            )\n",
    "        elif len(image.shape) == 3:  # RGB\n",
    "            num_channels = image.shape[2]\n",
    "            channels = [\n",
    "                ops.pad(image[:, :, channel_index], kernel, stride=1, padding_type=\"zero\")[0]\n",
    "                for channel_index in range(num_channels)\n",
    "            ]\n",
    "            padded_image = np.dstack(channel)\n",
    "\n",
    "        # fill in the output\n",
    "        stride = 1\n",
    "        output_image = list()\n",
    "        for image_channel_index in np.arange(num_channels):\n",
    "            transformed_channel = list()\n",
    "            channel = (\n",
    "                padded_image[:, :, image_channel_index]\n",
    "                if num_channels > 1\n",
    "                else padded_image\n",
    "            )\n",
    "            kernel_h, _ = kernel.shape\n",
    "            # iterate over the rows and columns\n",
    "            starting_row_ndx = 0\n",
    "            while starting_row_ndx <= len(channel) - kernel_h:\n",
    "                # convolve the next row of this channel\n",
    "                next_channel_row = ops.slide_kernel_over_image(\n",
    "                    channel,\n",
    "                    kernel,\n",
    "                    starting_row_ndx,\n",
    "                    stride,\n",
    "                    apply=compute_rank,\n",
    "                )\n",
    "                # now, add the convolved row to the list\n",
    "                transformed_channel.append(next_channel_row)\n",
    "                # move to the next starting row for the filtering\n",
    "                starting_row_ndx += stride\n",
    "            output_image.append(transformed_channel)\n",
    "        # stack the channels, and return\n",
    "        return np.dstack(output_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
