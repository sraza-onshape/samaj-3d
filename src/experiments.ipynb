{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2\n",
    "#### Name: Syed Zain Raza\n",
    "#### CWID: 20011917"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: allow Jupyter to \"hot reload\" the Python modules I wrote, to avoid restarting the kernel after every change\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Teddy Stereo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Loading the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of ./teddy/teddyL.pgm: 375 x 450\n"
     ]
    }
   ],
   "source": [
    "left_img = ops.load_image(\n",
    "    \"./teddy/teddyL.pgm\",\n",
    "    return_grayscale=True,\n",
    "    return_array=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of ./teddy/teddyR.pgm: 375 x 450\n"
     ]
    }
   ],
   "source": [
    "right_img = ops.load_image(\n",
    "    \"./teddy/teddyR.pgm\",\n",
    "    return_grayscale=True,\n",
    "    return_array=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the Rank Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class RankTransform2D:\n",
    "    @staticmethod\n",
    "    def transform(\n",
    "        image: np.ndarray,\n",
    "        filter_side_length: int,\n",
    "        do_logging: bool = False,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Perform rank filtering on an image.\n",
    "\n",
    "        The goal is to produce a new image where each cell value\n",
    "        represents the \"rank\" of the corresponding pixel in the input\n",
    "        (i.e., the index of said pixel in a sorted list of itself &\n",
    "        the neighboring pixel values).\n",
    "\n",
    "        Parameters:\n",
    "            image(np.ndarray): in case its RGB, the transform will be\n",
    "                                per channel. Please pass the image in\n",
    "                                channels-last format.\n",
    "            filter_side_length(int): this is k. The size of each local neighborhood\n",
    "                              will be kxk. Please pass an odd value > 0.\n",
    "            do_logging(bool): enables print statements to display intermediate\n",
    "                              values during execution\n",
    "        \n",
    "        Returns: np.ndarray: the transformed image\n",
    "        \"\"\"\n",
    "        ### HELPER(S)\n",
    "        def compute_rank(\n",
    "            channel: np.ndarray,\n",
    "            kernel: np.ndarray,\n",
    "            row_index: int,\n",
    "            col_index: int,\n",
    "        ) -> float:\n",
    "            \"\"\"\n",
    "            Computes the rank of 1 local window of the image.\n",
    "\n",
    "            Parameters:\n",
    "                channel(array-like): one of the channels of the input image\n",
    "                kernel(array-like): tells us the size of the window \n",
    "                row_index, col_index: int: the coordinates of the upper left corner\n",
    "                                            of the block of pixels being ranked\n",
    "\n",
    "            Returns: int: the rank of the center pixel of the window\n",
    "            \"\"\"\n",
    "            # A: define useful vars\n",
    "            kernel_h, kernel_w = kernel.shape\n",
    "            # B: get the block of pixels needed for the convolution\n",
    "            block_of_pixels = channel[\n",
    "                row_index : (kernel_h + row_index),\n",
    "                col_index : (kernel_w + col_index)\n",
    "            ]\n",
    "            # C: count the of # higher than the center\n",
    "            center_val = block_of_pixels[kernel_h // 2, kernel_w // 2]\n",
    "            if do_logging:\n",
    "                print(f\"I think that {center_val} is at the center of {block_of_pixels}\")\n",
    "            transformed_block = np.where(block_of_pixels < center_val, 1, 0)\n",
    "            if do_logging:\n",
    "                print(f\"Transformed block <{block_of_pixels}> into: <{transformed_block}>\")\n",
    "            return np.sum(transformed_block)\n",
    "\n",
    "        ### DRIVER\n",
    "        # data validation\n",
    "        assert isinstance(image, np.ndarray) \n",
    "        assert image.shape > (0, 0)\n",
    "        assert isinstance(filter_side_length, int)\n",
    "        assert filter_side_length > 0 and filter_side_length % 2 == 1\n",
    "\n",
    "        # make a copy of the img, padded - will be an intermediate repr\n",
    "        kernel = np.ones((filter_side_length, filter_side_length))\n",
    "        num_channels = -1\n",
    "        if len(image.shape) == 2:  # grayscale\n",
    "            num_channels = 1\n",
    "            padded_image, _, _ = ops.pad(\n",
    "                image, kernel, stride=1, padding_type=\"zero\"\n",
    "            )\n",
    "        elif len(image.shape) == 3:  # RGB\n",
    "            num_channels = image.shape[2]\n",
    "            channels = [\n",
    "                ops.pad(image[:, :, channel_index], kernel, stride=1, padding_type=\"zero\")[0]\n",
    "                for channel_index in range(num_channels)\n",
    "            ]\n",
    "            padded_image = np.dstack(channels)\n",
    "\n",
    "        # fill in the output\n",
    "        stride = 1\n",
    "        output_image = list()\n",
    "        for image_channel_index in np.arange(num_channels):\n",
    "            transformed_channel = list()\n",
    "            channel = (\n",
    "                padded_image[:, :, image_channel_index]\n",
    "                if num_channels > 1\n",
    "                else padded_image\n",
    "            )\n",
    "            kernel_h, _ = kernel.shape\n",
    "            # iterate over the rows and columns\n",
    "            starting_row_ndx = 0\n",
    "            while starting_row_ndx <= len(channel) - kernel_h:\n",
    "                # convolve the next row of this channel\n",
    "                next_channel_row = ops.slide_kernel_over_image(\n",
    "                    channel,\n",
    "                    kernel,\n",
    "                    starting_row_ndx,\n",
    "                    stride,\n",
    "                    apply=compute_rank,\n",
    "                )\n",
    "                # now, add the convolved row to the list\n",
    "                transformed_channel.append(next_channel_row)\n",
    "                # move to the next starting row for the filtering\n",
    "                starting_row_ndx += stride\n",
    "            output_image.append(transformed_channel)\n",
    "        # stack the channels, and return\n",
    "        return np.dstack(output_image).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: Rank Transform the Left View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_left = RankTransform2D.transform(\n",
    "    left_img, filter_side_length=5, do_logging=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check to make sure the rank transform produced a new image of the same dimensions as the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_left.shape == left_img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Computing a Disparity Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "import functools\n",
    "from typing import Literal, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from util import ops\n",
    "from util.ops import SimilarityMeasure\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SimpleStereoDisparityMap:\n",
    "    \"\"\"\n",
    "    Computes a disparity map between two images based on the\n",
    "    assumption that they are:\n",
    "        1) of the same shape, \n",
    "        2) grayscale,\n",
    "        3) and already rectified.\n",
    "    \"\"\"\n",
    "    left_image: np.ndarray\n",
    "    right_image: np.ndarray\n",
    "    stride: int = 1\n",
    "    padding_type: Union[Literal[\"zero\"], Literal[\"repeat\"]] = \"zero\"\n",
    "\n",
    "    def compute(\n",
    "        self: SimpleStereoDisparityMap,\n",
    "        similarity_measure: SimilarityMeasure,\n",
    "        rank_transform_filter_side_length: int,\n",
    "        window_size: int,\n",
    "        do_logging: bool = False,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute pixel-wise disparity by searching in the second image \n",
    "        the most similar patch along the same row in the first image.\n",
    "        \"\"\"\n",
    "        ### HELPER(S)\n",
    "        def _fetch_image_patches(\n",
    "            ptr: np.ndarray,  # aka the padded, rank transformed repr of the right image\n",
    "            left_pixel_block: np.ndarray,\n",
    "            starting_row_index: int,\n",
    "            ptr_starting_col_index: int,\n",
    "        ) -> int:\n",
    "            \"\"\"\n",
    "            TODO[add docstring]\n",
    "\n",
    "            Args:\n",
    "                channel: 2D array - one of the channels of the input image\n",
    "                kernel: 2D array representing the parameters to use\n",
    "                starting_row_index, ptr_starting_col_index: int: the coordinates of the upper left corner\n",
    "                                    of the block of pixels being convolved\n",
    "\n",
    "            Returns: int: TODO\n",
    "            \"\"\"\n",
    "            # form a list of tuple(patch, center_pixel_column)\n",
    "            kernel_h, kernel_w = left_pixel_block.shape\n",
    "            right_image_starting_col_indices = np.arange(\n",
    "                ptr_starting_col_index, ptr.shape[1] - kernel_w, self.stride\n",
    "            )\n",
    "            if do_logging:\n",
    "                print(\n",
    "                    f\"Starting to fetch patches starting at indices: {right_image_starting_col_indices}.\"\n",
    "                )\n",
    "            patches = np.zeros((\n",
    "                right_image_starting_col_indices.shape[0],\n",
    "                kernel_h, \n",
    "                kernel_w\n",
    "            ))\n",
    "            center_col_indices = np.zeros(right_image_starting_col_indices.shape[0])\n",
    "            iter_num = 0\n",
    "            for right_image_starting_col_ndx in right_image_starting_col_indices:\n",
    "                patch = ptr[\n",
    "                    starting_row_index : starting_row_index + kernel_h,\n",
    "                    right_image_starting_col_ndx : right_image_starting_col_ndx\n",
    "                    + kernel_w,\n",
    "                ]\n",
    "                patches[iter_num, :, :] = patch\n",
    "\n",
    "                right_image_center_col_index = right_image_starting_col_ndx + (kernel_w // 2)\n",
    "                center_col_indices[iter_num] = right_image_center_col_index\n",
    "\n",
    "                iter_num += 1\n",
    "            # for each patch in the row of the right image, compute the SAD with the same block from the first\n",
    "            compute_sad = functools.partial(\n",
    "                ops.compute_similarity,\n",
    "                mode=similarity_measure,\n",
    "                arr1=left_pixel_block\n",
    "            )\n",
    "            hypothesis_similarities = np.apply_along_axis(\n",
    "                lambda arr2: compute_sad(arr2=arr2),\n",
    "                axis=0,\n",
    "                arr=patches,\n",
    "            )\n",
    "            assert hypothesis_similarities.shape == center_col_indices.shape, (\n",
    "                f\"Expected hypothesis_similarities to have a shape of: {center_col_indices.shape}, \",\n",
    "                f\"but actual shape is: {hypothesis_similarities.shape}\",\n",
    "            )\n",
    "            # at the end, choose the patch of the right image with the most similarity\n",
    "            best_center_col_hypothesis = center_col_indices[np.argmin(hypothesis_similarities)]\n",
    "            return best_center_col_hypothesis\n",
    "\n",
    "        ### DRIVER\n",
    "        # data validations\n",
    "        assert self.left_image.shape == self.right_image.shape, (\n",
    "            f\"Image shape mismatch between left <{self.left_image.shape}>\"\n",
    "            f\" and right <{self.right_image.shape}> views.\"\n",
    "        )\n",
    "        assert len(self.left_image.shape) == 2, (\n",
    "            f\"Expected a 2D array representing a grayscale image, \"\n",
    "            f\"actual number of channels is: {self.left_image.shape[2]}.\"\n",
    "        )\n",
    "        # rank transform both images\n",
    "        transformed_left = RankTransform2D.transform(self.left_image, rank_transform_filter_side_length)\n",
    "        transformed_right = RankTransform2D.transform(self.right_image, rank_transform_filter_side_length)\n",
    "        # create a square kernel of all 1's using the given window size\n",
    "        kernel = np.ones((window_size, window_size))\n",
    "        # pad both images\n",
    "        padded_transformed_left, _, _ = ops.pad(\n",
    "            transformed_left,\n",
    "            kernel,\n",
    "            stride=self.stride,\n",
    "            padding_type=self.padding_type\n",
    "        )\n",
    "        ptl = padded_transformed_left\n",
    "        padded_transformed_right, _, _ = ops.pad(\n",
    "            transformed_right,\n",
    "            kernel,\n",
    "            stride=self.stride,\n",
    "            padding_type=self.padding_type\n",
    "        )\n",
    "        ptr = padded_transformed_right\n",
    "        assert (\n",
    "            ptl.shape == ptr.shape\n",
    "        ), f\"Shape mismatch after padding: {ptl.shape} != {ptr.shape}\"\n",
    "        # create the output image\n",
    "        output_image = list()\n",
    "        kernel_h, kernel_w = kernel.shape\n",
    "        # for every row in the first image\n",
    "        for starting_row_ndx in np.arange(0, ptl.shape[0] - kernel_h, self.stride):\n",
    "            output_image_row = list()\n",
    "            for left_image_starting_col_ndx in np.arange(0, ptl.shape[1] - kernel_w, self.stride):\n",
    "                left_pixel_block = padded_transformed_left[\n",
    "                    starting_row_ndx : starting_row_ndx + kernel_h,\n",
    "                    left_image_starting_col_ndx : left_image_starting_col_ndx + kernel_w,\n",
    "                ]\n",
    "                left_block_center_pixel_index = starting_row_ndx + (kernel_h // 2)\n",
    "                # slide the kernel over the same row in the second image\n",
    "                best_center_col_hypothesis = ops.slide_kernel_over_image(\n",
    "                    padded_transformed_right,\n",
    "                    kernel=left_pixel_block,\n",
    "                    row_index=starting_row_ndx,\n",
    "                    stride=self.stride,\n",
    "                    apply=_fetch_image_patches,\n",
    "                )\n",
    "                assert (\n",
    "                    left_block_center_pixel_index\n",
    "                    <= best_center_col_hypothesis\n",
    "                    < ptr.shape[1]\n",
    "                ), (\n",
    "                    f\"Impossible location of right correspondence is column index {best_center_col_hypothesis} \",\n",
    "                    f\"is not between [{left_block_center_pixel_index}, {ptr.shape[1]}).\",\n",
    "                )\n",
    "                disparity = np.abs(best_center_col_hypothesis - left_block_center_pixel_index)\n",
    "                if do_logging:\n",
    "                    print(f\"Upper left corner of patch has coords: ({starting_row_ndx}, {left_image_starting_col_ndx}).\")\n",
    "                    print(f\"Window size is {window_size}, so center is at col index: {left_block_center_pixel_index}\")\n",
    "                assert disparity >= 0, f\"disparity of {disparity} is too low\"\n",
    "                assert disparity <= 63, f\"disparity of {disparity} is too high\"\n",
    "                output_image_row.append(disparity)\n",
    "            output_image.append(output_image_row)\n",
    "        # final checks\n",
    "        output = np.array(output_image)\n",
    "        assert output.shape == self.left_image.shape, f\"Shape mismatch {output.shape} != {self.left_image.shape}\"\n",
    "        return output\n",
    "\n",
    "    @classmethod\n",
    "    def compute_and_visualize(\n",
    "        cls: SimpleStereoDisparityMap,\n",
    "        image1: np.ndarray,\n",
    "        image2: np.ndarray,\n",
    "        similarity_measure: SimilarityMeasure,\n",
    "        rank_transform_filter_side_length: int,\n",
    "        window_size: int,\n",
    "        scene_name: str,\n",
    "        do_logging: bool = False,\n",
    "        stride: int = 1,\n",
    "        padding_type: Union[Literal[\"zero\"], Literal[\"repeat\"]] = \"zero\",\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Convenience wrapper that both computes and plots\n",
    "        1 disparity map for a pair of stereo images.\n",
    "        TODO\n",
    "        \"\"\"\n",
    "        mapper = cls(\n",
    "            left_image=image1,\n",
    "            right_image=image2,\n",
    "            stride=stride,\n",
    "            padding_type=padding_type,\n",
    "        )\n",
    "        disparity_map = mapper.compute(\n",
    "            similarity_measure=similarity_measure,\n",
    "            rank_transform_filter_side_length=rank_transform_filter_side_length,\n",
    "            window_size=window_size,\n",
    "            do_logging=do_logging,\n",
    "        )\n",
    "        plt.imshow(disparity_map)\n",
    "        plt.title(f\"Left and Right View Disparity for \\\"{scene_name}\\\"\")\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: compare both the implementations of 3x3 window and 15x15 against the same GT map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the `SimpleStereoDisparityMap` with a 3x3 Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to fetch patches starting at indices: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
      " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
      " 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
      " 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
      " 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n",
      " 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395\n",
      " 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413\n",
      " 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431\n",
      " 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448].\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (3,3) (449,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mSimpleStereoDisparityMap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_and_visualize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mleft_img\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mright_img\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mSimilarityMeasure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSAD\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrank_transform_filter_side_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscene_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTeddy Bear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_logging\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzero\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 201\u001b[0m, in \u001b[0;36mSimpleStereoDisparityMap.compute_and_visualize\u001b[0;34m(cls, image1, image2, similarity_measure, rank_transform_filter_side_length, window_size, scene_name, do_logging, stride, padding_type)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;124;03mConvenience wrapper that both computes and plots\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03m1 disparity map for a pair of stereo images.\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;124;03mTODO\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    195\u001b[0m mapper \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\n\u001b[1;32m    196\u001b[0m     left_image\u001b[38;5;241m=\u001b[39mimage1,\n\u001b[1;32m    197\u001b[0m     right_image\u001b[38;5;241m=\u001b[39mimage2,\n\u001b[1;32m    198\u001b[0m     stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[1;32m    199\u001b[0m     padding_type\u001b[38;5;241m=\u001b[39mpadding_type,\n\u001b[1;32m    200\u001b[0m )\n\u001b[0;32m--> 201\u001b[0m disparity_map \u001b[38;5;241m=\u001b[39m \u001b[43mmapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43msimilarity_measure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msimilarity_measure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrank_transform_filter_side_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrank_transform_filter_side_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_logging\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_logging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(disparity_map)\n\u001b[1;32m    208\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLeft and Right View Disparity for \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mscene_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[19], line 149\u001b[0m, in \u001b[0;36mSimpleStereoDisparityMap.compute\u001b[0;34m(self, similarity_measure, rank_transform_filter_side_length, window_size, do_logging)\u001b[0m\n\u001b[1;32m    147\u001b[0m left_block_center_pixel_index \u001b[38;5;241m=\u001b[39m starting_row_ndx \u001b[38;5;241m+\u001b[39m (kernel_h \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# slide the kernel over the same row in the second image\u001b[39;00m\n\u001b[0;32m--> 149\u001b[0m best_center_col_hypothesis \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mslide_kernel_over_image\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadded_transformed_right\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkernel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_pixel_block\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrow_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstarting_row_ndx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapply\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fetch_image_patches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    157\u001b[0m     left_block_center_pixel_index\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m best_center_col_hypothesis\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis not between [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mleft_block_center_pixel_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mptr\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    163\u001b[0m )\n\u001b[1;32m    164\u001b[0m disparity \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(best_center_col_hypothesis \u001b[38;5;241m-\u001b[39m left_block_center_pixel_index)\n",
      "File \u001b[0;32m~/Downloads/dev/courses/Stevens/CS-532/src/util/ops.py:225\u001b[0m, in \u001b[0;36mslide_kernel_over_image\u001b[0;34m(channel, kernel, row_index, stride, apply)\u001b[0m\n\u001b[1;32m    222\u001b[0m starting_col_ndx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m starting_col_ndx \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(channel[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m-\u001b[39m kernel_w:\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;66;03m# compute the convolution\u001b[39;00m\n\u001b[0;32m--> 225\u001b[0m     conv_block_of_pixels \u001b[38;5;241m=\u001b[39m \u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchannel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstarting_col_ndx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;66;03m# add it to the output\u001b[39;00m\n\u001b[1;32m    227\u001b[0m     conv_channel_row\u001b[38;5;241m.\u001b[39mappend(conv_block_of_pixels)\n",
      "Cell \u001b[0;32mIn[19], line 90\u001b[0m, in \u001b[0;36mSimpleStereoDisparityMap.compute.<locals>._fetch_image_patches\u001b[0;34m(ptr, left_pixel_block, starting_row_index, ptr_starting_col_index)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# for each patch in the row of the right image, compute the SAD with the same block from the first\u001b[39;00m\n\u001b[1;32m     85\u001b[0m compute_sad \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[1;32m     86\u001b[0m     ops\u001b[38;5;241m.\u001b[39mcompute_similarity,\n\u001b[1;32m     87\u001b[0m     mode\u001b[38;5;241m=\u001b[39msimilarity_measure,\n\u001b[1;32m     88\u001b[0m     arr1\u001b[38;5;241m=\u001b[39mleft_pixel_block\n\u001b[1;32m     89\u001b[0m )\n\u001b[0;32m---> 90\u001b[0m hypothesis_similarities \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_along_axis\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43marr2\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_sad\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marr2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43marr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m hypothesis_similarities\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m center_col_indices\u001b[38;5;241m.\u001b[39mshape, (\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected hypothesis_similarities to have a shape of: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcenter_col_indices\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut actual shape is: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhypothesis_similarities\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     98\u001b[0m )\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# at the end, choose the patch of the right image with the most similarity\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/dev/courses/Stevens/CS-532/src/env/lib/python3.11/site-packages/numpy/lib/shape_base.py:379\u001b[0m, in \u001b[0;36mapply_along_axis\u001b[0;34m(func1d, axis, arr, *args, **kwargs)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    377\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot apply_along_axis when any iteration dimensions are 0\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    378\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 379\u001b[0m res \u001b[38;5;241m=\u001b[39m asanyarray(\u001b[43mfunc1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43minarr_view\u001b[49m\u001b[43m[\u001b[49m\u001b[43mind0\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    381\u001b[0m \u001b[38;5;66;03m# build a buffer for storing evaluations of func1d.\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# remove the requested axis, and add the new ones on the end.\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;66;03m# laid out so that each write is contiguous.\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;66;03m# for a tuple index inds, buff[inds] = func1d(inarr_view[inds])\u001b[39;00m\n\u001b[1;32m    385\u001b[0m buff \u001b[38;5;241m=\u001b[39m zeros(inarr_view\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m res\u001b[38;5;241m.\u001b[39mshape, res\u001b[38;5;241m.\u001b[39mdtype)\n",
      "Cell \u001b[0;32mIn[19], line 91\u001b[0m, in \u001b[0;36mSimpleStereoDisparityMap.compute.<locals>._fetch_image_patches.<locals>.<lambda>\u001b[0;34m(arr2)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# for each patch in the row of the right image, compute the SAD with the same block from the first\u001b[39;00m\n\u001b[1;32m     85\u001b[0m compute_sad \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[1;32m     86\u001b[0m     ops\u001b[38;5;241m.\u001b[39mcompute_similarity,\n\u001b[1;32m     87\u001b[0m     mode\u001b[38;5;241m=\u001b[39msimilarity_measure,\n\u001b[1;32m     88\u001b[0m     arr1\u001b[38;5;241m=\u001b[39mleft_pixel_block\n\u001b[1;32m     89\u001b[0m )\n\u001b[1;32m     90\u001b[0m hypothesis_similarities \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mapply_along_axis(\n\u001b[0;32m---> 91\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m arr2: \u001b[43mcompute_sad\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marr2\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     92\u001b[0m     axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     93\u001b[0m     arr\u001b[38;5;241m=\u001b[39mpatches,\n\u001b[1;32m     94\u001b[0m )\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m hypothesis_similarities\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m center_col_indices\u001b[38;5;241m.\u001b[39mshape, (\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected hypothesis_similarities to have a shape of: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcenter_col_indices\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut actual shape is: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhypothesis_similarities\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     98\u001b[0m )\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# at the end, choose the patch of the right image with the most similarity\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/dev/courses/Stevens/CS-532/src/util/ops.py:106\u001b[0m, in \u001b[0;36mcompute_similarity\u001b[0;34m(mode, arr1, arr2)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m### DRIVER\u001b[39;00m\n\u001b[1;32m    100\u001b[0m measure_funcs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    101\u001b[0m     SimilarityMeasure\u001b[38;5;241m.\u001b[39mSSD: _compute_ssd,\n\u001b[1;32m    102\u001b[0m     SimilarityMeasure\u001b[38;5;241m.\u001b[39mNCC: _compute_ncc,\n\u001b[1;32m    103\u001b[0m     SimilarityMeasure\u001b[38;5;241m.\u001b[39mCOS: _compute_cosine_similarity,\n\u001b[1;32m    104\u001b[0m     SimilarityMeasure\u001b[38;5;241m.\u001b[39mSAD: _compute_sum_absolute_difference,\n\u001b[1;32m    105\u001b[0m }\n\u001b[0;32m--> 106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmeasure_funcs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marr2\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/dev/courses/Stevens/CS-532/src/util/ops.py:97\u001b[0m, in \u001b[0;36mcompute_similarity.<locals>._compute_sum_absolute_difference\u001b[0;34m(arr1, arr2)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_compute_sum_absolute_difference\u001b[39m(arr1: np\u001b[38;5;241m.\u001b[39mndarray, arr2: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m     96\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Output array has a shape of (1,).\"\"\"\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39mlinalg(\u001b[43marr1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43marr2\u001b[49m), \u001b[38;5;28mord\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3,3) (449,) "
     ]
    }
   ],
   "source": [
    "SimpleStereoDisparityMap.compute_and_visualize(\n",
    "    left_img,\n",
    "    right_img,\n",
    "    SimilarityMeasure.SAD,\n",
    "    rank_transform_filter_side_length=5,\n",
    "    window_size=3,\n",
    "    scene_name=\"Teddy Bear\",\n",
    "    do_logging=True,\n",
    "    stride=1,\n",
    "    padding_type=\"zero\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
